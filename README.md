Report on the Optimization for Data Science project

===============================================================

This repo contains several implementations of novel SFO optimization methods. Among them:

Gradient Descend;
Stochastic Gradient Descend;
SARAH;
SARAH+;
SPIDER;
SPIDERBoost;
SPIDERBoost-m;
SVRG.
=============================================================== 

Note: all the implementations are written in Python using Numba. You need to modify the main.py file in the Numba installation in order to use the code, otherwise a downgrade of Numpy is necessary.
